# 风险评估: Story 3.3 - LLM回答生成与流式输出

日期: 2025-01-07  
审查人: Quinn (Test Architect)

## 执行摘要

- **总风险数**: 8
- **关键风险**: 2 (Score 9)
- **高风险**: 2 (Score 6)
- **中风险**: 3 (Score 4)
- **低风险**: 1 (Score 2)
- **风险评分**: 66/100

## 关键风险需要立即关注

### 1. [TECH-001] 无效 conversationId 导致外键约束错误

**评分: 9 (关键)**
**概率**: 高 - 前端可能保存已删除的conversationId
**影响**: 高 - 导致500错误，用户无法使用问答功能
**检测方法**: 生产环境发现实际错误

**缓解措施**:
- ✅ 已实施: 在 `route.ts` 中添加conversationId验证逻辑
- ✅ 已验证: 无效ID时自动创建新对话
- ✅ 已测试: 从生产日志确认修复生效

**残留风险**: 低 - 已完全缓解

**负责人**: dev  
**时间线**: ✅ 已完成 (2025-01-07)

---

### 2. [SEC-001] LLM API密钥泄露风险

**评分: 9 (关键)**
**概率**: 中 - 配置错误或代码暴露
**影响**: 高 - API密钥被盗，产生高额费用
**受影响组件**:
- `llm.config.ts`
- 环境变量配置
- LLM Repository实现

**缓解措施**:
- ✅ 密钥存储在环境变量中
- ✅ 不在代码中硬编码
- ❌ 缺少: 密钥轮换机制
- ❌ 缺少: API调用速率监控告警

**建议行动**:
1. 添加异常API调用量告警（>1000/小时）
2. 实施密钥定期轮换策略（每30天）
3. 添加IP白名单限制（生产环境）

**残留风险**: 中 - 需要额外安全措施

**负责人**: dev + ops  
**时间线**: 下个Sprint

---

## 高风险

### 3. [PERF-001] LLM响应超时导致用户体验下降

**评分: 6 (高)**
**概率**: 中 - LLM API偶尔慢响应
**影响**: 中 - 用户等待超过10秒，可能放弃

**当前实现**:
- 无明确超时设置
- 依赖LLM API默认超时
- 流式响应可能长时间阻塞

**缓解建议**:
- 添加30秒总体超时
- 首字节5秒超时
- 超时后显示友好错误信息

**测试策略**:
- 模拟慢LLM响应
- 验证超时处理逻辑

---

### 4. [DATA-001] 对话历史Token超限

**评分: 6 (高)**
**概率**: 中 - 长对话可能超过限制
**影响**: 中 - 生成失败或质量下降

**当前实现**:
- ✅ 有Token估算逻辑
- ✅ 有截断机制
- ⚠️ 截断策略简单（直接删除一半历史）

**优化建议**:
- 智能截断：保留最近对话 + 最相关历史
- 添加Token使用监控
- 对话摘要功能（长对话压缩）

---

## 中风险

### 5. [REL-001] 流式响应中断导致部分回答

**评分: 4 (中)**
**概率**: 低 - 网络稳定时较少发生
**影响**: 中 - 用户看到不完整回答

**当前实现**:
- ✅ 有错误处理
- ✅ 部分回答会保存
- ⚠️ 无明确用户提示

**改进建议**:
- 在UI显示"回答未完成"标记
- 提供"重新生成"按钮

---

### 6. [MNT-001] Prompt模板硬编码

**评分: 4 (中)**
**概率**: 高 - 当前实现就是硬编码
**影响**: 低 - 修改Prompt需要代码变更

**建议**:
- 将Prompt模板移至配置文件
- 支持A/B测试不同Prompt
- 版本化Prompt模板

---

### 7. [TECH-002] 智能路由未实现

**评分: 4 (中)**
**概率**: 高 - AC6标注为未完成
**影响**: 低 - 成本优化不足

**当前状态**: 使用固定提供商
**影响**: 无法根据复杂度选择合适模型

**建议**:
- 实施AC6智能路由
- 预计节省30-40% LLM成本

---

## 低风险

### 8. [OPS-001] 缺少详细监控指标

**评分: 2 (低)**
**概率**: 高 - 当前监控较基础
**影响**: 低 - 难以优化性能

**建议添加监控**:
- LLM响应延迟分布 (P50/P95/P99)
- Token消耗统计
- 流式chunk延迟
- 错误率和类型分布

---

## 风险分布

### 按类别

| 类别 | 关键 | 高 | 中 | 低 |
|------|------|----|----|-----|
| 技术风险 (TECH) | 1 | 0 | 1 | 0 |
| 安全风险 (SEC) | 1 | 0 | 0 | 0 |
| 性能风险 (PERF) | 0 | 1 | 0 | 0 |
| 数据风险 (DATA) | 0 | 1 | 0 | 0 |
| 可靠性风险 (REL) | 0 | 0 | 1 | 0 |
| 可维护性风险 (MNT) | 0 | 0 | 1 | 0 |
| 运维风险 (OPS) | 0 | 0 | 0 | 1 |

### 按组件

| 组件 | 风险数 | 最高评分 |
|------|--------|----------|
| answerService.ts | 3 | 9 (TECH-001) |
| LLM配置 | 2 | 9 (SEC-001) |
| promptBuilder.ts | 2 | 4 (MNT-001) |
| route.ts | 1 | 6 (PERF-001) |

---

## 基于风险的测试策略

### 优先级1: 关键风险测试

- ✅ 无效conversationId场景（已修复验证）
- 测试API密钥配置错误场景
- 验证环境变量缺失处理

### 优先级2: 高风险测试

- LLM超时场景
- Token超限场景
- 长对话历史处理

### 优先级3: 中风险测试

- 网络中断恢复
- Prompt模板修改影响
- 不同复杂度问题处理

---

## 风险接受标准

### 必须修复才能生产 (关键风险)

- ✅ TECH-001: 无效conversationId - 已修复
- ⚠️ SEC-001: API密钥安全 - 需要增强监控

### 可以部署但需监控 (高风险)

- PERF-001: 超时处理 - 添加监控
- DATA-001: Token管理 - 记录超限事件

### 接受的风险 (中低风险)

- MNT-001, TECH-002, OPS-001: 后续优化
- REL-001: 通过监控和用户反馈改进

---

## 监控需求

### 实时告警

- LLM API调用失败率 > 5%
- 平均响应时间 > 10秒
- Token超限事件 > 10次/小时
- 异常API调用量 > 1000次/小时

### 日常监控

- LLM成本统计
- Token消耗分布
- 问答准确率反馈
- 用户满意度评分

---

## 总结与建议

### 当前风险状态: ✅ 可控

- 关键风险已缓解1个，需要增强1个
- 高风险通过监控可控
- 中低风险可以接受

### 立即行动项

1. ✅ 修复无效conversationId问题 - **已完成**
2. ⚠️ 添加API密钥使用量监控告警 - **高优先级**
3. 添加LLM响应超时控制 - **中优先级**

### 下Sprint优化项

1. 实施智能路由（AC6）
2. Prompt模板配置化
3. 完善监控Dashboard

### 风险趋势

- ✅ 稳定性: 外键约束问题已修复
- ⚠️ 安全性: 需要增强API密钥保护
- ✅ 性能: 基本满足要求，可持续优化
