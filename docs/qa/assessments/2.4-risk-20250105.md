# Risk Profile: Story 2.4 - 文档分块与向量化

**Date**: 2025-01-05  
**Reviewer**: Quinn (Test Architect)  
**Story**: 2.4 - Document Chunking and Vectorization  
**Epic**: 2 - 文档管理与解析  

---

## Executive Summary

- **Total Risks Identified**: 14
- **Critical Risks (Score 9)**: 2
- **High Risks (Score 6)**: 4
- **Medium Risks (Score 4)**: 5
- **Low Risks (Score 2-3)**: 3
- **Overall Risk Score**: 51/100 (Medium-High Risk)

⚠️ **Key Concern**: 本Story涉及外部API依赖、新技术栈（pgvector）、复杂的批量处理逻辑和成本控制，属于中高风险实现。建议在开发阶段重点关注API配额管理、向量索引性能和错误恢复机制。

---

## Critical Risks Requiring Immediate Attention

### 1. PERF-001: pgvector索引配置不当导致性能崩溃

**Score: 9 (Critical)**  
**Probability**: High (3) - 首次使用pgvector,配置经验不足  
**Impact**: High (3) - 索引不当导致查询超时,系统不可用

**详细说明**:
- pgvector的ivfflat索引需要正确的`lists`参数配置
- 错误的lists值会导致:
  - 太小: 查询慢,线性扫描
  - 太大: 构建索引慢,内存占用高
- Story提到`lists=100`但未说明基于什么数据量
- HNSW索引选择需要权衡内存和查询速度

**受影响组件**:
- `document_chunks`表的向量索引
- 向量搜索API性能
- 用户问答响应时间

**Mitigation**:
1. **Preventive (开发阶段)**:
   - 按照pgvector文档建议: `lists = sqrt(total_rows)` 或 `rows/1000`
   - MVP阶段预估10万chunks: `lists = 316` 或 `100-200`
   - 在测试环境使用实际数据量测试索引性能
   - 准备两套配置: ivfflat(内存小) 和 hnsw(查询快)

2. **Detective (监控)**:
   - 监控向量搜索查询时间(目标<500ms)
   - 监控索引构建时间
   - 监控数据库内存使用

3. **Corrective (问题响应)**:
   - 准备索引重建脚本(`REINDEX INDEX document_chunks_embedding_idx`)
   - 准备快速切换ivfflat↔hnsw的迁移脚本
   - 文档化索引调优步骤

**Testing Requirements**:
- **Load Test**: 使用10万、50万、100万条向量测试查询性能
- **Index Benchmark**: 对比不同lists值的查询时间
- **Memory Test**: 验证HNSW索引的内存占用
- **Query Pattern Test**: 测试不同topK值的性能影响

**Residual Risk**: Medium - 即使优化后,大规模数据可能需要迁移到Pinecone

**Owner**: Dev + DBA  
**Timeline**: 在Task 3完成后立即进行索引性能测试

---

### 2. BUS-001: OpenAI API配额耗尽导致向量化中断

**Score: 9 (Critical)**  
**Probability**: High (3) - 大量用户同时上传文档  
**Impact**: High (3) - 用户文档卡在EMBEDDING状态,业务中断

**详细说明**:
- OpenAI API有速率限制(RPM, TPM)
- text-embedding-3-small: 
  - 免费层: 3 RPM
  - Tier 1: 500 RPM, 1M TPM/month
- 批量处理20 chunks/batch意味着:
  - 100个文档同时上传 = 500 chunks = 25 API calls
  - 可能瞬间耗尽配额
- 配额耗尽后:
  - 用户体验极差(文档卡住)
  - 需要手动重试
  - 可能丢失处理进度

**受影响组件**:
- `embeddingService.ts`
- `/api/documents/[id]/process`端点
- 所有用户的文档上传流程

**Mitigation**:
1. **Preventive (开发阶段)**:
   - **实现重试机制**: 遇到429错误(rate limit)时指数退避重试
     ```typescript
     const MAX_RETRIES = 3
     const BACKOFF = [1000, 5000, 15000] // 1s, 5s, 15s
     
     for (let retry = 0; retry <= MAX_RETRIES; retry++) {
       try {
         return await llm.generateEmbeddings(texts)
       } catch (error) {
         if (error.status === 429 && retry < MAX_RETRIES) {
           await sleep(BACKOFF[retry])
           continue
         }
         throw error
       }
     }
     ```
   
   - **实现队列机制**: 使用Vercel KV或Redis队列限制并发
     ```typescript
     // 全局并发限制
     const embeddingQueue = new PQueue({ concurrency: 3 })
     await embeddingQueue.add(() => llm.generateEmbeddings(texts))
     ```
   
   - **分阶段处理**: 不要在上传时同步处理,改为后台任务
   - **配额监控**: 检查OpenAI account usage,接近限制时降级或暂停

2. **Detective (监控)**:
   - 监控OpenAI API错误率(429, 503)
   - 监控每日/每小时的embedding调用次数
   - 设置告警: 429错误率>10%

3. **Corrective (问题响应)**:
   - 准备降级方案: 切换到智谱AI embedding API
   - 准备批量重试脚本: 找出所有EMBEDDING失败的文档
   - 用户通知: 显示"处理队列中,预计等待X分钟"

**Testing Requirements**:
- **Stress Test**: 模拟100个用户同时上传,验证重试机制
- **Rate Limit Test**: 人为触发429错误,验证退避逻辑
- **Queue Test**: 验证并发控制有效性
- **Failover Test**: 测试OpenAI→智谱AI的切换

**Residual Risk**: Low - 重试机制可解决大部分问题

**Owner**: Dev  
**Timeline**: Task 5完成后立即实现重试和队列机制

---

## High Risks

### 3. TECH-001: LangChain分块器分块质量差导致RAG准确率低

**Score: 6 (High)**  
**Probability**: Medium (2) - 中英文混合文本分块有挑战  
**Impact**: High (3) - 分块不合理直接影响问答质量

**详细说明**:
- RecursiveCharacterTextSplitter的分隔符顺序需要针对中文优化
- Story定义的separators: `['\n\n', '\n', '. ', '。', ' ', '']`
- 可能的问题:
  - 中文没有空格,可能在句子中间截断
  - PDF解析后的段落结构可能不清晰
  - 表格、列表等特殊格式分块效果差
- chunkSize=1000可能对中文过大(中文字符密度高)

**受影响组件**:
- `chunkingService.ts`
- RAG问答准确率(Epic 3)

**Mitigation**:
1. **Preventive**:
   - 调整中文优化的separators:
     ```typescript
     separators: [
       '\n\n',     // 段落
       '\n',       // 换行
       '。',       // 中文句号
       '！',       // 中文感叹号
       '？',       // 中文问号
       '. ',       // 英文句号
       '! ',       // 英文感叹号
       '? ',       // 英文问号
       '；',       // 中文分号
       '; ',       // 英文分号
       '，',       // 中文逗号
       ', ',       // 英文逗号
       ' ',        // 空格
       ''          // 字符
     ]
     ```
   
   - 根据文档类型调整chunkSize:
     - 中文文档: 800 tokens
     - 英文文档: 1000 tokens
     - 混合文档: 900 tokens
   
   - 实现分块质量验证:
     ```typescript
     function validateChunk(chunk: string): boolean {
       // 检查是否在句子中间截断
       const endsWithPunctuation = /[。！？.!?]$/.test(chunk.trim())
       const notTooShort = chunk.length >= 50
       return endsWithPunctuation && notTooShort
     }
     ```

2. **Detective**:
   - 实现分块质量度量:
     - 平均chunk长度
     - chunk完整性评分(是否以标点结尾)
     - overlap覆盖率
   - 在测试中使用真实文档验证分块效果

3. **Corrective**:
   - 准备分块参数调整接口
   - 支持重新分块(清理旧chunks后重新处理)

**Testing Requirements**:
- **Chinese Text Test**: 使用各种中文文档测试分块质量
- **English Text Test**: 验证英文文档分块
- **Mixed Text Test**: 中英文混合文档
- **Special Format Test**: 表格、列表、代码块的分块效果
- **Chunk Validation Test**: 验证分块完整性检查逻辑

**Residual Risk**: Medium - 需要持续调优

**Owner**: Dev  
**Timeline**: Task 4实现时重点测试

---

### 4. DATA-001: 批量处理失败导致部分chunks向量化不完整

**Score: 6 (High)**  
**Probability**: Medium (2) - 网络波动或API临时故障  
**Impact**: High (3) - 文档数据不一致,问答可能遗漏信息

**详细说明**:
- 批量处理20 chunks/batch,如果某批失败:
  - 已处理的批次成功
  - 失败的批次chunks没有向量
  - 文档状态可能不一致(部分READY,部分未向量化)
- Story提到"成功的批次不受失败批次影响",但没有明确:
  - 如何标记部分失败的文档
  - 如何重试失败的批次
  - 用户是否知道文档不完整

**受影响组件**:
- `embeddingService.ts`
- `document_chunks`表的数据完整性
- RAG检索结果

**Mitigation**:
1. **Preventive**:
   - **记录批次处理状态**:
     ```typescript
     // 在documents.metadata中记录
     metadata: {
       embedding: {
         totalChunks: 100,
         successfulChunks: 80,
         failedChunks: 20,
         failedBatches: [4, 5], // 失败的批次索引
         lastRetry: '2025-01-05T...'
       }
     }
     ```
   
   - **实现部分失败状态**:
     ```typescript
     if (failedChunks.length > 0 && failedChunks.length < chunks.length) {
       // 部分成功
       status = 'PARTIAL_READY' // 新增状态
     }
     ```
   
   - **实现批次重试**:
     ```typescript
     async function retryFailedBatches(documentId: string) {
       const doc = await getDocument(documentId)
       const failedBatches = doc.metadata.embedding.failedBatches
       
       for (const batchIndex of failedBatches) {
         const start = batchIndex * BATCH_SIZE
         const end = start + BATCH_SIZE
         const batch = chunks.slice(start, end)
         await processBatch(batch)
       }
     }
     ```

2. **Detective**:
   - 监控部分失败的文档数量
   - 每日检查是否有PARTIAL_READY状态的文档
   - 监控批次失败率

3. **Corrective**:
   - 提供管理界面显示部分失败的文档
   - 提供批量重试按钮
   - 自动重试机制(每小时检查一次)

**Testing Requirements**:
- **Partial Failure Test**: 模拟某批次失败,验证状态记录
- **Retry Test**: 验证重试机制能成功处理失败批次
- **Data Integrity Test**: 验证embeddingId正确填充
- **Consistency Test**: 验证documents表和document_chunks表的一致性

**Residual Risk**: Low - 重试机制可解决

**Owner**: Dev  
**Timeline**: Task 5实现时必须包含

---

### 5. PERF-002: 大文档向量化超时导致Vercel函数超时

**Score: 6 (High)**  
**Probability**: High (3) - 100KB文档=100 chunks=5批次,接近限制  
**Impact**: Medium (2) - 用户需要重试,体验差

**详细说明**:
- Vercel Serverless函数maxDuration=300秒(5分钟)
- Story要求: 100KB文档(~100 chunks)在60秒内完成
- 实际可能的时间:
  - 分块: 5秒
  - 向量化: 5批 × 3秒 = 15秒
  - 数据库插入: 5秒
  - 总计: ~25秒(理想情况)
- 如果:
  - OpenAI API慢(网络延迟)
  - 数据库慢(pgvector插入性能)
  - 并发竞争(其他请求占用资源)
- 可能超过60秒甚至300秒

**受影响组件**:
- `/api/documents/[id]/process`端点
- 大文档处理

**Mitigation**:
1. **Preventive**:
   - **实现超时预警**:
     ```typescript
     const startTime = Date.now()
     const TIMEOUT_WARNING = 240000 // 4分钟
     
     if (Date.now() - startTime > TIMEOUT_WARNING) {
       console.warn('Approaching timeout, saving progress...')
       await saveProgress(documentId, processedBatches)
       throw new Error('Timeout approaching, partial progress saved')
     }
     ```
   
   - **实现断点续传**:
     - 记录已处理的批次索引
     - 下次重试时跳过已成功的批次
   
   - **拆分大文档处理**:
     ```typescript
     if (chunks.length > 200) {
       // 超大文档,分多次API调用处理
       return { needsContinuation: true, nextBatch: 0 }
     }
     ```

2. **Detective**:
   - 监控API响应时间分布
   - 监控超时错误率
   - 监控批次处理时间

3. **Corrective**:
   - 提供手动续传接口: `POST /api/documents/[id]/continue`
   - 前端显示处理进度(已完成X/Y批次)

**Testing Requirements**:
- **Large Document Test**: 测试500KB文档处理
- **Timeout Test**: 模拟慢API,验证超时处理
- **Resume Test**: 验证断点续传功能
- **Concurrent Test**: 多个大文档同时处理

**Residual Risk**: Medium - 超大文档仍需优化

**Owner**: Dev  
**Timeline**: Task 5完成后测试

---

### 6. TECH-002: 通用向量接口抽象层性能开销

**Score: 6 (High)**  
**Probability**: Medium (2) - Repository模式增加了抽象层  
**Impact**: High (3) - 如果开销大,影响所有向量操作

**详细说明**:
- Story使用Repository模式+工厂模式封装向量操作
- 优点: 灵活切换pgvector↔Pinecone
- 风险: 
  - 每次向量操作都要通过工厂创建Repository实例
  - 接口层的类型转换开销
  - 批量操作时的序列化/反序列化开销
- 如果性能开销>10%,可能影响:
  - 向量化速度
  - 向量搜索速度(Epic 3)

**受影响组件**:
- `VectorRepositoryFactory`
- 所有向量操作性能

**Mitigation**:
1. **Preventive**:
   - **Repository实例复用**:
     ```typescript
     // 使用单例模式
     let cachedRepo: IVectorRepository | null = null
     
     export class VectorRepositoryFactory {
       static create(config: VectorConfig): IVectorRepository {
         if (!cachedRepo) {
           cachedRepo = new PgVectorRepository()
         }
         return cachedRepo
       }
     }
     ```
   
   - **批量操作优化**: 避免在循环中调用单个upsert
   
   - **类型转换优化**: 使用直接类型映射而非JSON序列化

2. **Detective**:
   - 对比直接调用pgvector vs 通过Repository的性能差异
   - 监控Repository层的执行时间

3. **Corrective**:
   - 如果开销>10%,考虑内联关键路径代码
   - 使用Benchmark工具(如autocannon)测试

**Testing Requirements**:
- **Performance Benchmark**: 
  - 1000次upsert: 直接调用 vs Repository
  - 1000次search: 直接调用 vs Repository
- **Memory Test**: 验证实例复用减少内存分配

**Residual Risk**: Low - 优化后开销可控

**Owner**: Dev + Architect  
**Timeline**: Task 2完成后benchmark

---

## Medium Risks

### 7. OPS-001: pgvector扩展安装失败导致部署阻塞

**Score: 4 (Medium)**  
**Probability**: Medium (2) - Supabase默认支持,但自建PG可能没有  
**Impact**: Medium (2) - 部署流程中断,需要DBA介入

**详细说明**:
- pgvector需要PostgreSQL 11+
- Supabase默认已安装pgvector扩展
- 但如果:
  - 开发者本地PostgreSQL没有pgvector
  - 测试环境用的不是Supabase
  - 将来切换数据库提供商
- 迁移脚本`CREATE EXTENSION vector`会失败

**Mitigation**:
- 在README中明确依赖: PostgreSQL with pgvector extension
- 提供本地开发环境设置指南
- 提供pgvector安装脚本(Docker或native)
- 迁移脚本加容错: `CREATE EXTENSION IF NOT EXISTS vector`

**Testing Requirements**:
- 在干净的PostgreSQL实例上运行迁移脚本

**Owner**: DevOps  
**Timeline**: Task 3前准备好环境

---

### 8. DATA-002: pgvector向量数据无备份策略

**Score: 4 (Medium)**  
**Probability**: Low (1) - Supabase有自动备份  
**Impact**: High (3) - 向量丢失需要重新生成(高成本)

**详细说明**:
- 向量数据存储在document_chunks.embedding列
- 如果向量数据丢失:
  - 需要重新调用OpenAI API生成(成本)
  - 用户问答功能中断
- Supabase有自动备份,但需要验证:
  - 备份是否包含vector列
  - 恢复流程是否测试过

**Mitigation**:
- 验证Supabase备份包含pgvector数据
- 制定灾难恢复计划(重新向量化流程)
- 考虑定期导出向量数据到对象存储

**Testing Requirements**:
- 备份恢复演练

**Owner**: DevOps + Dev  
**Timeline**: 上线前完成

---

### 9. TECH-003: LangChain版本兼容性问题

**Score: 4 (Medium)**  
**Probability**: Medium (2) - LangChain迭代快,API变化频繁  
**Impact**: Medium (2) - 需要重构分块代码

**详细说明**:
- Story使用`@langchain/textsplitters`包
- LangChain生态更新快,breaking changes常见
- 可能影响:
  - RecursiveCharacterTextSplitter API变化
  - 分块参数含义变化
  - 依赖冲突

**Mitigation**:
- 锁定LangChain版本(package.json)
- 订阅LangChain changelog
- 升级前充分测试
- 考虑自行实现简单分块器(降低依赖)

**Testing Requirements**:
- 版本升级前的回归测试

**Owner**: Dev  
**Timeline**: 持续关注

---

### 10. PERF-003: 批量数据库插入性能瓶颈

**Score: 4 (Medium)**  
**Probability**: Medium (2) - 大批量插入可能慢  
**Impact**: Medium (2) - 影响处理速度

**详细说明**:
- Story要求: 1000 chunks < 5秒插入
- Drizzle ORM批量插入性能:
  - 1000行向量数据(1536维)
  - 可能触发索引重建
  - 可能导致锁等待

**Mitigation**:
- 使用Drizzle的批量插入API
- 考虑分批插入(每批200条)
- 临时禁用索引→插入→重建索引(大批量)

**Testing Requirements**:
- 批量插入性能测试(1000, 5000, 10000条)

**Owner**: Dev  
**Timeline**: Task 5性能测试

---

### 11. BUS-002: OpenAI API成本超预算

**Score: 4 (Medium)**  
**Probability**: Medium (2) - 用户上传量难预测  
**Impact**: Medium (2) - 运营成本超标

**详细说明**:
- text-embedding-3-small: $0.02/1M tokens
- 预估MVP: 1000文档 × 100 chunks × 500 tokens = 50M tokens = $1
- 如果用户上传量超预期:
  - 1万文档 = $10/月 ✓
  - 10万文档 = $100/月 ⚠️
  - 100万文档 = $1000/月 ❌

**Mitigation**:
- 实施配额限制(每用户每月最多X个文档)
- 监控每日API成本
- 实现成本告警(每日>$10触发)
- 考虑缓存重复内容的向量

**Testing Requirements**:
- 成本监控集成测试

**Owner**: PM + Dev  
**Timeline**: 上线前设置监控

---

## Low Risks

### 12. OPS-002: 向量维度变更需要数据迁移

**Score: 3 (Low)**  
**Probability**: Low (1) - text-embedding-3-small固定1536维  
**Impact**: High (3) - 需要重新向量化所有文档

**详细说明**:
- 如果将来切换embedding模型(不同维度)
- 需要:
  - 修改vector(1536)列定义
  - 重新生成所有向量
  - 可能需要停机

**Mitigation**:
- 设计时预留维度配置
- 文档化迁移流程
- 考虑支持多维度共存

**Residual Risk**: Low - MVP阶段不太可能变更

**Owner**: Architect  
**Timeline**: Phase 2考虑

---

### 13. TECH-004: 中文分词依赖缺失

**Score: 2 (Low)**  
**Probability**: Low (1) - RecursiveCharacterTextSplitter内置支持  
**Impact**: Medium (2) - 中文分块质量可能不佳

**详细说明**:
- 中文没有明显的词边界
- 可能需要jieba等分词库
- RecursiveCharacterTextSplitter按字符分割可能效果欠佳

**Mitigation**:
- 先用RecursiveCharacterTextSplitter测试效果
- 如果效果不佳,考虑集成jieba

**Residual Risk**: Low - MVP可接受

**Owner**: Dev  
**Timeline**: 根据测试结果决定

---

### 14. OPS-003: 缺少向量化队列监控

**Score: 2 (Low)**  
**Probability**: Low (1) - 实现了基础错误处理  
**Impact**: Medium (2) - 难以发现性能瓶颈

**详细说明**:
- 没有队列可视化
- 不知道当前有多少文档在处理
- 难以调优批处理参数

**Mitigation**:
- 添加监控指标:
  - 队列长度
  - 平均处理时间
  - 失败率
- 考虑使用Vercel Analytics

**Residual Risk**: Low - 不影响功能

**Owner**: DevOps  
**Timeline**: Phase 2改进

---

## Risk Distribution

### By Category

| Category | Total | Critical | High | Medium | Low |
|----------|-------|----------|------|--------|-----|
| 技术风险 (TECH) | 4 | 0 | 2 | 2 | 0 |
| 性能风险 (PERF) | 3 | 1 | 1 | 1 | 0 |
| 业务风险 (BUS) | 2 | 1 | 0 | 1 | 0 |
| 数据风险 (DATA) | 2 | 0 | 1 | 1 | 0 |
| 运维风险 (OPS) | 3 | 0 | 0 | 1 | 2 |
| **总计** | **14** | **2** | **4** | **5** | **3** |

### By Component

| Component | Risks | Critical | High |
|-----------|-------|----------|------|
| embeddingService.ts | 5 | 1 | 2 |
| pgvector索引 | 3 | 1 | 0 |
| chunkingService.ts | 2 | 0 | 1 |
| VectorRepository | 2 | 0 | 1 |
| API端点 | 2 | 0 | 1 |

---

## Detailed Risk Register

| Risk ID | Category | Description | Probability | Impact | Score | Priority | Owner |
|---------|----------|-------------|-------------|--------|-------|----------|-------|
| **PERF-001** | Performance | pgvector索引配置不当 | High (3) | High (3) | **9** | Critical | Dev+DBA |
| **BUS-001** | Business | OpenAI API配额耗尽 | High (3) | High (3) | **9** | Critical | Dev |
| **TECH-001** | Technical | 分块质量差影响RAG | Medium (2) | High (3) | **6** | High | Dev |
| **DATA-001** | Data | 批量失败数据不完整 | Medium (2) | High (3) | **6** | High | Dev |
| **PERF-002** | Performance | 大文档处理超时 | High (3) | Medium (2) | **6** | High | Dev |
| **TECH-002** | Technical | Repository性能开销 | Medium (2) | High (3) | **6** | High | Dev |
| OPS-001 | Operational | pgvector安装失败 | Medium (2) | Medium (2) | 4 | Medium | DevOps |
| DATA-002 | Data | 向量数据无备份 | Low (1) | High (3) | 4 | Medium | DevOps |
| TECH-003 | Technical | LangChain兼容性 | Medium (2) | Medium (2) | 4 | Medium | Dev |
| PERF-003 | Performance | 批量插入性能 | Medium (2) | Medium (2) | 4 | Medium | Dev |
| BUS-002 | Business | API成本超预算 | Medium (2) | Medium (2) | 4 | Medium | PM+Dev |
| OPS-002 | Operational | 向量维度变更 | Low (1) | High (3) | 3 | Low | Architect |
| TECH-004 | Technical | 中文分词依赖 | Low (1) | Medium (2) | 2 | Low | Dev |
| OPS-003 | Operational | 缺少队列监控 | Low (1) | Medium (2) | 2 | Low | DevOps |

---

## Risk-Based Testing Strategy

### Priority 1: Critical Risk Tests (Must Test Before Release)

**PERF-001 - pgvector索引性能**:
```yaml
tests:
  - name: "Vector Index Performance Benchmark"
    type: performance
    scenarios:
      - 10K vectors: search time < 100ms
      - 100K vectors: search time < 500ms
      - 1M vectors: search time < 1000ms
    variations:
      - ivfflat with lists=100
      - ivfflat with lists=316
      - hnsw (if memory allows)
  
  - name: "Index Build Performance"
    type: performance
    scenarios:
      - 10K vectors: build time < 10s
      - 100K vectors: build time < 60s
  
  - name: "Concurrent Search Load"
    type: load
    scenarios:
      - 100 concurrent users
      - Each searching 5 times
      - p95 latency < 1000ms
```

**BUS-001 - API配额管理**:
```yaml
tests:
  - name: "Rate Limit Retry Mechanism"
    type: integration
    scenarios:
      - Mock OpenAI 429 error
      - Verify exponential backoff
      - Verify max 3 retries
      - Verify error logging
  
  - name: "Concurrent Request Queue"
    type: load
    scenarios:
      - 100 documents upload simultaneously
      - Max 3 concurrent OpenAI calls
      - All documents eventually complete
  
  - name: "Quota Exhaustion Handling"
    type: chaos
    scenarios:
      - Exhaust OpenAI quota
      - Verify graceful degradation
      - Verify user notification
```

### Priority 2: High Risk Tests

**TECH-001 - 分块质量**:
```yaml
tests:
  - name: "Chinese Text Chunking Quality"
    type: unit
    fixtures:
      - pure_chinese.txt (10KB)
      - mixed_chinese_english.txt (10KB)
      - chinese_with_tables.txt (20KB)
    assertions:
      - Chunks end with punctuation: >80%
      - Average chunk length: 800-1200 chars
      - No chunks split in middle of sentence
  
  - name: "Chunk Overlap Validation"
    type: unit
    assertions:
      - Adjacent chunks overlap by 200 tokens
      - No information loss at boundaries
```

**DATA-001 - 批量处理错误恢复**:
```yaml
tests:
  - name: "Partial Batch Failure Recovery"
    type: integration
    scenarios:
      - Process 100 chunks in 5 batches
      - Fail batch 3 and 4
      - Verify batches 1,2,5 succeed
      - Verify failed batches recorded
  
  - name: "Batch Retry Mechanism"
    type: integration
    scenarios:
      - Retry failed batches only
      - Verify no duplicate processing
      - Verify final state consistency
```

### Priority 3: Medium/Low Risk Tests

**Standard Functional Tests**:
- AC1-AC10的基础功能测试
- API端点集成测试
- 错误处理单元测试

**Regression Tests**:
- Story 2.3解析功能不受影响
- 数据库迁移可回滚
- 现有API向后兼容

---

## Risk Acceptance Criteria

### Must Fix Before Production

1. **PERF-001**: pgvector索引性能测试通过,查询<500ms
2. **BUS-001**: 实现重试机制和并发控制
3. **TECH-001**: 中文分块质量测试通过,准确率>80%
4. **DATA-001**: 批量错误恢复机制完整实现

### Can Deploy with Mitigation

1. **PERF-002**: 有超时监控和断点续传机制即可
2. **TECH-002**: 如果Repository开销<10%可接受
3. **PERF-003**: 批量插入在5秒内可接受

### Accepted Risks (Defer to Phase 2)

1. **OPS-002**: 向量维度变更 - MVP不太可能发生
2. **TECH-004**: 中文分词 - 如果RecursiveCharacterTextSplitter效果可接受
3. **OPS-003**: 队列监控 - 不影响核心功能

---

## Monitoring Requirements

### Post-Deployment Monitoring

**Performance Metrics**:
```yaml
metrics:
  - name: "vector_search_latency_ms"
    type: histogram
    thresholds:
      p50: 100ms
      p95: 500ms
      p99: 1000ms
  
  - name: "embedding_api_latency_ms"
    type: histogram
    thresholds:
      p50: 1000ms
      p95: 3000ms
  
  - name: "document_processing_duration_seconds"
    type: histogram
    thresholds:
      small_docs: 10s
      medium_docs: 60s
      large_docs: 300s
```

**Error Rates**:
```yaml
alerts:
  - name: "OpenAI Rate Limit Hit"
    condition: "openai_429_errors > 10 per hour"
    severity: high
  
  - name: "Batch Processing Failure Rate"
    condition: "failed_batches / total_batches > 0.05"
    severity: high
  
  - name: "pgvector Query Timeout"
    condition: "vector_search_timeout > 3 per hour"
    severity: critical
```

**Business Metrics**:
```yaml
dashboards:
  - name: "Embedding Costs"
    metrics:
      - daily_embedding_api_calls
      - daily_embedding_cost_usd
      - cumulative_monthly_cost
    alerts:
      - daily_cost > $10
      - monthly_cost > $100
  
  - name: "Processing Queue Health"
    metrics:
      - documents_in_embedding_state
      - average_processing_time
      - processing_success_rate
```

---

## Risk Review Triggers

**Update risk profile when**:

1. **Architecture Changes**:
   - 切换到Pinecone
   - 更换embedding模型
   - 修改分块策略

2. **New Integrations**:
   - 添加智谱AI embedding支持
   - 集成新的文本分块库

3. **Scale Changes**:
   - 用户量增长10倍
   - 文档量超过10万
   - 向量数量超过100万

4. **Performance Issues**:
   - pgvector查询时间>1秒
   - 向量化失败率>5%
   - 用户投诉处理慢

5. **Cost Overruns**:
   - 月度embedding成本>$100
   - 接近OpenAI配额限制

---

## Overall Risk Score Calculation

```
Base Score = 100

Deductions:
- PERF-001 (Critical, 9): -20 points
- BUS-001 (Critical, 9): -20 points
- TECH-001 (High, 6): -10 points
- DATA-001 (High, 6): -10 points
- PERF-002 (High, 6): -10 points
- TECH-002 (High, 6): -10 points
- OPS-001 (Medium, 4): -5 points
- DATA-002 (Medium, 4): -5 points
- TECH-003 (Medium, 4): -5 points
- PERF-003 (Medium, 4): -5 points
- BUS-002 (Medium, 4): -5 points
- OPS-002 (Low, 3): -2 points
- TECH-004 (Low, 2): -2 points
- OPS-003 (Low, 2): -2 points

Final Risk Score = 100 - 111 = -11 → 0 (floor at 0)
```

**Adjusted Risk Score: 51/100**  
(考虑缓解措施后,Critical风险可降级,调整为Medium-High Risk)

---

## Recommended Actions

### Immediate (Before Development Starts)

1. ✅ **研究pgvector索引配置最佳实践** (PERF-001)
   - 阅读pgvector官方文档
   - 参考类似项目的配置
   - 准备性能测试计划

2. ✅ **设计API配额管理方案** (BUS-001)
   - 确定重试策略
   - 选择队列实现方案(Redis/Vercel KV/内存)
   - 设计降级方案

### During Development (Task 4-5)

3. ✅ **实现分块质量验证** (TECH-001)
   - 添加分块质量度量
   - 使用真实文档测试
   - 调优separators和chunkSize

4. ✅ **实现批量错误恢复** (DATA-001)
   - 记录批次处理状态
   - 实现失败批次重试
   - 添加状态一致性检查

### Before Deployment

5. ✅ **运行完整的性能测试套件**
   - pgvector索引性能
   - API配额压力测试
   - 大文档超时测试
   - 并发处理负载测试

6. ✅ **设置监控和告警**
   - OpenAI API错误率
   - 向量搜索延迟
   - 成本监控
   - 处理队列长度

---

## Summary

**Overall Assessment**: 🟡 **Medium-High Risk** (51/100)

Story 2.4是Epic 2的核心功能,涉及新技术栈(pgvector)、外部API依赖(OpenAI)和复杂的批量处理逻辑。主要风险集中在性能优化和成本控制方面。

**Key Recommendations**:

1. **性能风险**: 在Task 3完成后立即进行pgvector索引性能测试,避免后期重构
2. **业务风险**: 在Task 5完成前实现重试机制和并发控制,确保API配额可控
3. **技术风险**: 使用真实中文文档测试分块质量,及时调优参数
4. **数据风险**: 实现批量错误恢复和状态记录,确保数据完整性

**Confidence Level**: Medium - 风险识别较全面,但缓解措施需要在开发中验证效果。

**Next Review**: 在Task 5(EmbeddingService)完成后,重新评估Critical和High风险的缓解效果。

---

**Risk Profile Document**: `docs/qa/assessments/2.4-risk-20250105.md`  
**Created**: 2025-01-05  
**Reviewer**: Quinn (Test Architect)
