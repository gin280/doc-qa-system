# Epic 3: 智能问答与引用系统

**Epic ID**: 3  
**Epic名称**: 智能问答与引用系统  
**优先级**: P0 (MVP必须)  
**状态**: Ready  
**预计工期**: 3-4周

---

## Epic目标

实现核心的RAG智能问答功能,包括向量检索、LLM生成、精确引用、对话管理和导出功能。完成后,用户可以对文档提问并获得带引用的高质量回答,形成完整可用的MVP产品。

## 价值陈述

完成此Epic后:
- ✅ 用户可以针对文档提出自然语言问题
- ✅ 系统基于文档内容生成准确回答(准确率≥85%)
- ✅ 回答包含精确到段落的引用标注(引用准确率≥90%)
- ✅ 支持多轮对话,保持上下文连贯性
- ✅ 用户可以查看和导出对话历史
- ✅ **交付完整可用的MVP产品**

## 包含的用户故事

| Story ID | Story标题 | 优先级 | 预估工时 | 状态 |
|----------|-----------|--------|----------|------|
| 3.1 | 问答界面与输入处理 | P0 | 2天 | Draft |
| 3.2 | RAG向量检索实现 | P0 | 2天 | Draft |
| 3.3 | LLM回答生成与流式输出 | P0 | 3天 | Draft |
| 3.4 | 引用标注与溯源 | P0 | 3天 | Draft |
| 3.5 | 对话历史管理 | P1 | 2天 | Draft |
| 3.6 | 对话导出与分享 | P2 | 2天 | Draft |

**总计**: 6个Story,预计14天

## 验收标准

Epic完成的定义:

1. **问答核心功能**
   - [x] 用户可以针对文档提问
   - [x] 系统返回基于文档的回答
   - [x] 回答流式输出(打字机效果)
   - [x] 首字节响应时间 ≤ 3秒
   - [x] 完整回答时间 ≤ 10秒

2. **RAG质量**
   - [x] 向量检索相关性 > 0.7
   - [x] 问答准确率 ≥ 85%(基于用户满意度)
   - [x] 引用位置准确率 ≥ 90%
   - [x] 支持多轮对话(保持上下文)

3. **引用系统**
   - [x] 回答包含引用标记([1][2]等)
   - [x] 引用显示文档名称和位置
   - [x] 点击引用可跳转到原文
   - [x] 原文高亮显示引用段落

4. **对话管理**
   - [x] 对话自动保存
   - [x] 可以查看历史对话列表
   - [x] 支持搜索历史对话
   - [x] 可以删除对话记录
   - [x] 可以导出对话为Markdown

5. **性能和准确性**
   - [x] 问答准确率 ≥ 85%
   - [x] 引用准确率 ≥ 90%
   - [x] API可用性 ≥ 99.5%
   - [x] 向量检索时间 < 500ms

## 架构依赖

**前端**:
- React Markdown (Markdown渲染)
- Framer Motion (流式动画)
- SWR (数据缓存)

**后端**:
- LangChain.js (RAG流程编排)
- 多LLM适配器 (智谱/OpenAI/Claude/Gemini)
- Vercel AI SDK (流式响应)
- pgvector (向量检索,MVP)

**数据库**:
- Drizzle Schema: conversations, messages, citations

**缓存**:
- Upstash Redis (查询缓存,减少API调用)

**相关架构文档**:
- `docs/architecture.md#rag-implementation` - RAG完整实现
- `docs/architecture.md#query-processing-pipeline` - 查询处理流程
- `docs/architecture.md#llm-universal-interface` - 多LLM适配器
- `docs/architecture.md#vector-database-universal-interface` - 向量检索接口

## 技术风险

| 风险 | 影响 | 缓解措施 |
|------|------|----------|
| LLM响应慢 | 高 | 使用流式输出,首字节3秒内返回 |
| 问答准确率低 | 高 | 优化Prompt,调整检索参数,人工测试 |
| 引用定位不准 | 中 | 保留段落元数据,使用chunk_index定位 |
| LLM API费用高 | 中 | 使用智能路由,简单问题用便宜模型 |
| 多轮对话上下文丢失 | 中 | 限制历史轮次(10轮),token控制 |

## 依赖关系

**前置依赖**: 
- Epic 1 (用户认证)
- Epic 2 (文档向量数据)

**后续依赖**: 
- 无(本Epic完成即MVP交付)

## 成功指标

- 问答准确率 > 85%(用户满意度评分)
- 引用准确率 > 90%(引用指向正确段落)
- 首字节响应时间 < 3秒(P95)
- 完整回答时间 < 10秒(P95)
- 用户留存率 > 40%(7日留存)

---

## 开发顺序

建议按以下顺序开发(Story 3.1 → 3.2 → 3.3 → 3.4 → 3.5 → 3.6):

**第一周**:
1. Story 3.1 - 问答界面
2. Story 3.2 - RAG向量检索(核心)

**第二周**:
3. Story 3.3 - LLM回答生成(核心)

**第三周**:
4. Story 3.4 - 引用标注与溯源(核心)
5. Story 3.5 - 对话历史管理

**第四周**:
6. Story 3.6 - 对话导出
7. 端到端测试和准确率优化
8. MVP交付准备

## 技术亮点

### 1. 多LLM智能路由

使用多LLM适配器策略,根据任务复杂度和部署区域自动选择最优模型:

- **简单问答**: Gemini Flash(国际) / GLM-3-Turbo(国内) - 成本降低93%
- **复杂分析**: GPT-4 Turbo(国际) / GLM-4(国内) - 准确率最高
- **自动降级**: 4个提供商自动切换,确保高可用性

成本优化:
- 智能路由节省 ~$150/月
- 查询缓存减少30%重复调用
- 总LLM成本控制在 ~$353/月(国际) 或 ~$511/月(国内)

### 2. RAG优化策略

- **分块策略**: 1000 tokens/chunk, 200 tokens overlap
- **检索优化**: Top-K=5, 相似度阈值0.7
- **Prompt工程**: 明确指示只使用文档内容,标注来源
- **上下文控制**: 限制context window ≤2000 tokens

### 3. 引用精确定位

- 保留chunk_index和pageNumber元数据
- LLM回答中使用[1][2]标记引用
- 前端点击引用→滚动+高亮原文
- 引用准确率目标 ≥ 90%

---

## MVP交付检查清单

Epic 3完成后,进行MVP交付前检查:

**功能完整性**:
- [ ] 用户可以注册和登录(Epic 1)
- [ ] 用户可以上传并管理文档(Epic 2)
- [ ] 用户可以提问并获得带引用的回答(Epic 3)
- [ ] 所有核心功能正常工作

**性能达标**:
- [ ] 首屏加载 < 2秒
- [ ] 文档解析 < 30秒(10MB)
- [ ] 问答响应 < 3秒(首字节)
- [ ] 问答准确率 > 85%

**质量保证**:
- [ ] 单元测试覆盖率 > 70%
- [ ] 所有API集成测试通过
- [ ] E2E测试核心流程通过
- [ ] 无严重安全漏洞

**部署就绪**:
- [ ] 生产环境配置完成
- [ ] 数据库迁移完成
- [ ] 监控和日志配置完成
- [ ] 错误追踪(Sentry)配置完成

---

**Epic负责人**: Development Team  
**产品负责人**: Product Manager  
**创建日期**: 2025-01-03  
**最后更新**: 2025-01-03

