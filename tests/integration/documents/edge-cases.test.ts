/**
 * è¾¹ç•Œæƒ…å†µå¤„ç† - é›†æˆæµ‹è¯•
 * Story 4.5: AC1-AC5
 */
import { describe, it, expect, beforeAll, afterAll, beforeEach } from '@jest/globals'
import { db } from '@/lib/db'
import { documents, documentChunks } from '@/drizzle/schema'
import { eq } from 'drizzle-orm'
import { chunkDocument } from '@/services/documents/chunkingService'

describe('Edge Cases - Integration Tests', () => {
  const testUserId = 'test-user-edge-cases'
  let testDocumentIds: string[] = []

  beforeEach(() => {
    testDocumentIds = []
  })

  afterAll(async () => {
    // æ¸…ç†æµ‹è¯•æ•°æ®
    for (const docId of testDocumentIds) {
      await db.delete(documentChunks).where(eq(documentChunks.documentId, docId))
      await db.delete(documents).where(eq(documents.id, docId))
    }
  })

  describe('AC1 & AC2: ç©ºæ–‡æ¡£ç«¯åˆ°ç«¯å¤„ç†', () => {
    it('4.5-INT-001: ç©ºæ–‡æ¡£åº”è®¾ç½® FAILED çŠ¶æ€', async () => {
      // Arrange - åˆ›å»ºç©ºæ–‡æ¡£
      const [doc] = await db.insert(documents).values({
        userId: testUserId,
        filename: 'empty.txt',
        mimeType: 'text/plain',
        size: 0,
        storageKey: 'test/empty.txt',
        status: 'READY',
        metadata: { content: '' }
      }).returning()

      testDocumentIds.push(doc.id)

      // Act & Assert - åº”è¯¥æŠ›å‡ºé”™è¯¯
      await expect(chunkDocument(doc.id)).rejects.toThrow('æ–‡æ¡£å†…å®¹ä¸ºç©ºï¼Œæ— æ³•å¤„ç†')

      // Verify - æ£€æŸ¥æ•°æ®åº“çŠ¶æ€
      const [updatedDoc] = await db.select()
        .from(documents)
        .where(eq(documents.id, doc.id))

      expect(updatedDoc.status).toBe('FAILED')
      expect(updatedDoc.metadata).toHaveProperty('error')
      const metadata = updatedDoc.metadata as any
      expect(metadata.error.type).toBe('EMPTY_CONTENT')
      expect(metadata.error.message).toContain('æ–‡æ¡£å†…å®¹ä¸ºç©º')
    })

    it('4.5-INT-002: metadata åº”åŒ…å« EMPTY_CONTENT é”™è¯¯ç±»å‹', async () => {
      const [doc] = await db.insert(documents).values({
        userId: testUserId,
        filename: 'empty2.txt',
        mimeType: 'text/plain',
        size: 0,
        storageKey: 'test/empty2.txt',
        status: 'READY',
        metadata: { content: '' }
      }).returning()

      testDocumentIds.push(doc.id)

      try {
        await chunkDocument(doc.id)
      } catch (error) {
        // Expected to throw
      }

      const [updatedDoc] = await db.select()
        .from(documents)
        .where(eq(documents.id, doc.id))

      const metadata = updatedDoc.metadata as any
      expect(metadata.error).toBeDefined()
      expect(metadata.error.type).toBe('EMPTY_CONTENT')
      expect(metadata.error.timestamp).toBeDefined()
    })

    it('4.5-INT-003: ç©ºç™½æ–‡æ¡£åº”è¿”å›å‹å¥½é”™è¯¯', async () => {
      const [doc] = await db.insert(documents).values({
        userId: testUserId,
        filename: 'whitespace.txt',
        mimeType: 'text/plain',
        size: 10,
        storageKey: 'test/whitespace.txt',
        status: 'READY',
        metadata: { content: '   \n\t  ' }
      }).returning()

      testDocumentIds.push(doc.id)

      await expect(chunkDocument(doc.id)).rejects.toThrow('æ–‡æ¡£å†…å®¹ä¸ºç©ºï¼Œæ— æ³•å¤„ç†')

      const [updatedDoc] = await db.select()
        .from(documents)
        .where(eq(documents.id, doc.id))

      expect(updatedDoc.status).toBe('FAILED')
      const metadata = updatedDoc.metadata as any
      expect(metadata.error.type).toBe('EMPTY_CONTENT')
    })
  })

  describe('AC5: Unicode ç‰¹æ®Šå­—ç¬¦å¤„ç†', () => {
    it('4.5-INT-006: åº”è¯¥æ­£ç¡®å¤„ç†çº¯ä¸­æ–‡æ–‡æ¡£', async () => {
      const chineseContent = 'è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æ¡£ã€‚å†…å®¹åŒ…å«å®Œæ•´çš„ä¸­æ–‡å¥å­ã€‚'.repeat(50)
      
      const [doc] = await db.insert(documents).values({
        userId: testUserId,
        filename: 'chinese.txt',
        mimeType: 'text/plain',
        size: chineseContent.length,
        storageKey: 'test/chinese.txt',
        status: 'READY',
        metadata: { content: chineseContent }
      }).returning()

      testDocumentIds.push(doc.id)

      // Act
      const chunks = await chunkDocument(doc.id)

      // Assert
      expect(chunks.length).toBeGreaterThan(0)
      expect(chunks[0].content).toMatch(/[\u4e00-\u9fa5]/) // åŒ…å«ä¸­æ–‡
      expect(chunks[0].content).not.toContain('ï¿½') // æ— ä¹±ç 
    })

    it('4.5-INT-007: åº”è¯¥æ­£ç¡®å¤„ç† Emoji æ–‡æ¡£', async () => {
      const emojiContent = 'Hello ğŸ‘‹ World ğŸŒ Test ğŸš€ Document ğŸ“„ '.repeat(50)
      
      const [doc] = await db.insert(documents).values({
        userId: testUserId,
        filename: 'emoji.txt',
        mimeType: 'text/plain',
        size: emojiContent.length,
        storageKey: 'test/emoji.txt',
        status: 'READY',
        metadata: { content: emojiContent }
      }).returning()

      testDocumentIds.push(doc.id)

      const chunks = await chunkDocument(doc.id)

      expect(chunks.length).toBeGreaterThan(0)
      expect(chunks[0].content).toMatch(/ğŸ‘‹|ğŸŒ|ğŸš€|ğŸ“„/)
    })

    it('4.5-INT-008: åº”è¯¥æ­£ç¡®å¤„ç†ä¸­è‹±æ–‡æ··åˆæ–‡æ¡£', async () => {
      const mixedContent = 'Price: â‚¬100, ç‰ˆæƒ Copyright Â©2025, å–œæ¬¢ Love â™¥ '.repeat(50)
      
      const [doc] = await db.insert(documents).values({
        userId: testUserId,
        filename: 'mixed.txt',
        mimeType: 'text/plain',
        size: mixedContent.length,
        storageKey: 'test/mixed.txt',
        status: 'READY',
        metadata: { content: mixedContent }
      }).returning()

      testDocumentIds.push(doc.id)

      const chunks = await chunkDocument(doc.id)

      expect(chunks.length).toBeGreaterThan(0)
      expect(chunks[0].content).toMatch(/ç‰ˆæƒ/)
      expect(chunks[0].content).toMatch(/â‚¬|Â©|â™¥/)
    })

    it('4.5-INT-009: åº”è¯¥æ­£ç¡®å¤„ç†ç‰¹æ®Šç¬¦å·æ–‡æ¡£', async () => {
      const symbolContent = 'â„¢ Â® Â© â‚¬ Â£ Â¥ Â± Ã— Ã· â‰  â‰¤ â‰¥ '.repeat(50)
      
      const [doc] = await db.insert(documents).values({
        userId: testUserId,
        filename: 'symbols.txt',
        mimeType: 'text/plain',
        size: symbolContent.length,
        storageKey: 'test/symbols.txt',
        status: 'READY',
        metadata: { content: symbolContent }
      }).returning()

      testDocumentIds.push(doc.id)

      const chunks = await chunkDocument(doc.id)

      expect(chunks.length).toBeGreaterThan(0)
      // éªŒè¯ç‰¹æ®Šç¬¦å·è¢«æ­£ç¡®ä¿ç•™
      expect(chunks[0].content.length).toBeGreaterThan(0)
    })
  })

  describe('AC3: è¶…å¤§æ–‡æ¡£æˆªæ–­å¤„ç†', () => {
    it('4.5-INT-004: æˆªæ–­å metadata åº”åŒ…å«å®Œæ•´ä¿¡æ¯', async () => {
      // ç”Ÿæˆä¸€ä¸ªä¼šäº§ç”Ÿè¾ƒå¤šchunksçš„æ–‡æ¡£ï¼ˆä½†ä¸æ˜¯çœŸçš„15000ä¸ªï¼Œè€Œæ˜¯ä½¿ç”¨ä¸€ä¸ªä¸­ç­‰å¤§å°æ¥æµ‹è¯•é€»è¾‘ï¼‰
      // è¿™é‡Œä½¿ç”¨çº¦2000ä¸ªchunksçš„æ–‡æ¡£æ¥éªŒè¯metadataè®°å½•é€»è¾‘
      const largeContent = 'è¿™æ˜¯æµ‹è¯•å†…å®¹ã€‚'.repeat(50000) // çº¦50ä¸‡å­—ç¬¦ï¼Œä¼šäº§ç”Ÿè¾ƒå¤šchunks
      
      const [doc] = await db.insert(documents).values({
        userId: testUserId,
        filename: 'medium-large.txt',
        mimeType: 'text/plain',
        size: largeContent.length,
        storageKey: 'test/medium-large.txt',
        status: 'READY',
        metadata: { content: largeContent }
      }).returning()

      testDocumentIds.push(doc.id)

      // Act
      const chunks = await chunkDocument(doc.id)

      // Assert - éªŒè¯chunksæ•°é‡
      expect(chunks.length).toBeGreaterThan(0)
      expect(chunks.length).toBeLessThanOrEqual(10000)

      // å¦‚æœåŸå§‹æ•°é‡è¶…è¿‡10000ï¼ŒéªŒè¯metadataåŒ…å«æˆªæ–­ä¿¡æ¯
      const [updatedDoc] = await db.select()
        .from(documents)
        .where(eq(documents.id, doc.id))

      // éªŒè¯åŸºæœ¬ç»“æ„
      expect(updatedDoc.metadata).toBeDefined()
      
      // å¦‚æœè¢«æˆªæ–­ï¼ŒéªŒè¯metadataç»“æ„
      const metadata = updatedDoc.metadata as any
      if (metadata.chunking?.truncated) {
        expect(metadata.chunking.truncated).toBe(true)
        expect(metadata.chunking.originalChunksCount).toBeGreaterThan(10000)
        expect(metadata.chunking.storedChunksCount).toBe(10000)
      }
    })

    it('4.5-INT-005: ä¸­ç­‰å¤§å°æ–‡æ¡£åº”è¯¥æ­£å¸¸å¤„ç†ä¸æˆªæ–­', async () => {
      // ç”Ÿæˆä¸€ä¸ªä¸ä¼šè¶…è¿‡é™åˆ¶çš„æ–‡æ¡£
      const normalContent = 'è¿™æ˜¯æ­£å¸¸å¤§å°çš„æµ‹è¯•æ–‡æ¡£ã€‚'.repeat(1000) // çº¦1.2ä¸‡å­—ç¬¦ï¼Œä¸ä¼šè¶…è¿‡10000 chunks
      
      const [doc] = await db.insert(documents).values({
        userId: testUserId,
        filename: 'normal-size.txt',
        mimeType: 'text/plain',
        size: normalContent.length,
        storageKey: 'test/normal-size.txt',
        status: 'READY',
        metadata: { content: normalContent }
      }).returning()

      testDocumentIds.push(doc.id)

      // Act
      const chunks = await chunkDocument(doc.id)

      // Assert
      expect(chunks.length).toBeGreaterThan(0)
      expect(chunks.length).toBeLessThan(10000) // æ­£å¸¸å¤§å°ä¸åº”æ¥è¿‘é™åˆ¶

      // éªŒè¯æ²¡æœ‰æˆªæ–­æ ‡è®°
      const [updatedDoc] = await db.select()
        .from(documents)
        .where(eq(documents.id, doc.id))

      const metadata = updatedDoc.metadata as any
      expect(metadata.chunking?.truncated).toBeUndefined()
    })
  })

  // Note: çœŸæ­£çš„15000+ chunksæµ‹è¯•ä¼šè€—æ—¶å¾ˆé•¿ä¸”å ç”¨å¤§é‡èµ„æºï¼Œ
  // å› æ­¤è¿™é‡Œä½¿ç”¨ä¸­ç­‰å¤§å°çš„æ–‡æ¡£æ¥éªŒè¯æˆªæ–­é€»è¾‘çš„æ­£ç¡®æ€§ã€‚
  // å®Œæ•´çš„æ€§èƒ½å’Œæé™æµ‹è¯•åº”åœ¨æ€§èƒ½æµ‹è¯•å¥—ä»¶ä¸­è¿›è¡Œã€‚
})

